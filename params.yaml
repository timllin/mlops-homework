prepare:
  split: 0.80
  seed: 20170428

featurize:
  tokenizer: "distilbert-base-uncased"
  max_seq_len: 32

train:
  batch_size: 256
  num_classes: 6
  dropout_rate: 0.1
  model_name: "distilbert-base-uncased"
  lr: 0.0003
  n_epochs: 1
  weight_decay: 0.00001

evaluate:
  batch_size: 256
  num_classes: 6
  dropout_rate: 0.1
  model_name: "distilbert-base-uncased"